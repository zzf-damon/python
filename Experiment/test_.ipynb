{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''link_infos = pd.read_csv('data/gy_contest_link_info.txt', delimiter=';', dtype={'link_ID': object})\n",
    "print(link_infos.head(5))\n",
    "\n",
    "link_tops = pd.read_csv('data/gy_contest_link_top.txt', delimiter=';', dtype={'link_ID': object})\n",
    "print(link_tops.head(5))\n",
    "\n",
    "df = pd.read_csv('data/quaterfinal_gy_cmp_training_traveltime.txt', delimiter=';', dtype={'link_ID': object})\n",
    "print(df.head(5))\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "df['travel_time'].hist(bins=100, ax=axes[0])\n",
    "df['travel_time'] = np.log1p(df['travel_time'])\n",
    "df['travel_time'].hist(bins=100, ax=axes[1])\n",
    "plt.show()\n",
    "\n",
    "iris=datasets.load_iris()\n",
    "name=iris.feature_names\n",
    "iris=pd.DataFrame(iris.data)\n",
    "iris.columns=name\n",
    "def quantile_clip(group):\n",
    "    group.plot()\n",
    "    group[group < group.quantile(.05)] = group.quantile(.05)\n",
    "    group[group > group.quantile(.95)] = group.quantile(.95)\n",
    "    group.plot()\n",
    "    plt.show()\n",
    "    return group\n",
    "iris['sepal length (cm)'].transform(quantile_clip)'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time_interval_begin             link_ID\n",
      "0       4377906289869500514 2016-07-01 00:00:00\n",
      "1       4377906289869500514 2016-07-01 00:02:00\n",
      "2       4377906289869500514 2016-07-01 00:04:00\n",
      "3       4377906289869500514 2016-07-01 00:06:00\n",
      "4       4377906289869500514 2016-07-01 00:08:00\n",
      "...                     ...                 ...\n",
      "440635  4377906284525800514 2017-07-31 23:50:00\n",
      "440636  4377906284525800514 2017-07-31 23:52:00\n",
      "440637  4377906284525800514 2017-07-31 23:54:00\n",
      "440638  4377906284525800514 2017-07-31 23:56:00\n",
      "440639  4377906284525800514 2017-07-31 23:58:00\n",
      "\n",
      "[440640 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and datetime64[ns] columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6621012652b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'link_ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time_interval_begin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# to avoid incompat dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;31m# If argument passed to validate,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\conda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1143\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetimelike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_datetimelike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1145\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on object and datetime64[ns] columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/quaterfinal_gy_cmp_training_traveltime.txt', delimiter=';', dtype={'link_ID': object})\n",
    "link_df = pd.read_csv('data/gy_contest_link_info.txt', delimiter=';', dtype={'link_ID': object})\n",
    "\n",
    "date_range = pd.date_range(\"2016-07-01 00:00:00\", \"2016-07-31 23:58:00\", freq='2min').append(\n",
    "    pd.date_range(\"2017-04-01 00:00:00\", \"2017-07-31 23:58:00\", freq='2min'))\n",
    "\n",
    "new_index = pd.MultiIndex.from_product([link_df['link_ID'].unique(), date_range],\n",
    "                                       names={'link_ID','time_interval_begin'})\n",
    "df1 = pd.DataFrame(index=new_index).reset_index()\n",
    "print(df1)\n",
    "df3 = pd.merge(df, df1, on=['link_ID'], how='right')\n",
    "df3 = df3.loc[(df3['time_interval_begin'].dt.hour.isin([6, 7, 8, 13, 14, 15, 16, 17, 18]))]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df3 = df3.loc[~((df3['time_interval_begin'].dt.year == 2017) & (df3['time_interval_begin'].dt.month == 7) & (\n",
    "    df3['travel_time'].dt.hour.isin([8, 15, 18])))]\n",
    "\n",
    "df3['date'] = df3['time_interval_begin'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df3.loc[df3['travel_time'].isnull() == True].groupby('date')['link_ID'].count().plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''def date_trend(group):\n",
    "    tmp = group.groupby('date_hour').mean().reset_index()\n",
    "\n",
    "    def nan_helper(y):\n",
    "        return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "    y = tmp['travel_time'].values\n",
    "    nans, x = nan_helper(y)\n",
    "    if group.link_ID.values[0] in ['3377906282328510514', '3377906283328510514', '4377906280784800514',\n",
    "                                   '9377906281555510514']:\n",
    "        tmp['date_trend'] = group['travel_time'].median()\n",
    "    else:\n",
    "        regr = linear_model.LinearRegression()\n",
    "        regr.fit(x(~nans).reshape(-1, 1), y[~nans].reshape(-1, 1))\n",
    "        tmp['date_trend'] = regr.predict(tmp.index.values.reshape(-1, 1)).ravel()\n",
    "    group = pd.merge(group, tmp[['date_trend', 'date_hour']], on='date_hour', how='left')\n",
    "    plt.plot(tmp.index, tmp['date_trend'], 'o', tmp.index, tmp['travel_time'], 'ro')\n",
    "    plt.title(group.link_ID.values[0])\n",
    "    plt.show()\n",
    "    return group\n",
    "\n",
    "\n",
    "df['date_hour'] = df.time_interval_begin.map(lambda x: x.strftime('%Y-%m-%d-%H'))\n",
    "df = df.groupby('link_ID').apply(date_trend)\n",
    "df = df.drop(['date_hour', 'link_ID'], axis=1)\n",
    "df = df.reset_index()\n",
    "df = df.drop('level_1', axis=1)\n",
    "df['travel_time'] = df['travel_time'] - df['date_trend']\n",
    "\n",
    "\n",
    "def minute_trend(group):\n",
    "    tmp = group.groupby('hour_minute').mean().reset_index()\n",
    "    spl = UnivariateSpline(tmp.index, tmp['travel_time'].values, s=1, k=3)\n",
    "    tmp['minute_trend'] = spl(tmp.index)\n",
    "    plt.plot(tmp.index, spl(tmp.index), 'r', tmp.index, tmp['travel_time'], 'o')\n",
    "    plt.title(group.link_ID.values[0])\n",
    "    plt.show()\n",
    "    # print group.link_ID.values[0]\n",
    "    group = pd.merge(group, tmp[['minute_trend', 'hour_minute']], on='hour_minute', how='left')\n",
    "\n",
    "    return group\n",
    "\n",
    "df['hour_minute'] = df.time_interval_begin.map(lambda x: x.strftime('%H-%M'))\n",
    "df = df.groupby('link_ID').apply(minute_trend)\n",
    "\n",
    "df = df.drop(['hour_minute', 'link_ID'], axis=1)\n",
    "df = df.reset_index()\n",
    "df = df.drop('level_1', axis=1)\n",
    "df['travel_time'] = df['travel_time'] - df['minute_trend']\n",
    "\n",
    "\n",
    "link_infos = pd.read_csv('raw/gy_contest_link_info.txt', delimiter=';', dtype={'link_ID': object})\n",
    "link_tops = pd.read_csv('raw/gy_contest_link_top.txt', delimiter=';', dtype={'link_ID': object})\n",
    "link_tops['in_links'] = link_tops['in_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "link_tops['out_links'] = link_tops['out_links'].str.len().apply(lambda x: np.floor(x / 19))\n",
    "link_tops = link_tops.fillna(0)\n",
    "link_infos = pd.merge(link_infos, link_tops, on=['link_ID'], how='left')\n",
    "link_infos['links_num'] = link_infos[\"in_links\"].astype('str') + \",\" + link_infos[\"out_links\"].astype('str')\n",
    "link_infos['area'] = link_infos['length'] * link_infos['width']\n",
    "df = pd.merge(df, link_infos[['link_ID', 'length', 'width', 'links_num', 'area']], on=['link_ID'], how='left')\n",
    "\n",
    "df.loc[df['date'].isin(\n",
    "    ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "     '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 1\n",
    "\n",
    "df.loc[~df['date'].isin(\n",
    "    ['2017-04-02', '2017-04-03', '2017-04-04', '2017-04-29', '2017-04-30', '2017-05-01',\n",
    "     '2017-05-28', '2017-05-29', '2017-05-30']), 'vacation'] = 0\n",
    "\n",
    "df['minute'] = df['time_interval_begin'].dt.minute\n",
    "df['hour'] = df['time_interval_begin'].dt.hour\n",
    "df['day'] = df['time_interval_begin'].dt.day\n",
    "df['week_day'] = df['time_interval_begin'].map(lambda x: x.weekday() + 1)\n",
    "df['month'] = df['time_interval_begin'].dt.month\n",
    "\n",
    "def mean_time(group):\n",
    "    group['link_ID_en'] = group['travel_time'].mean()\n",
    "    return group\n",
    "\n",
    "df = df.groupby('link_ID').apply(mean_time)\n",
    "sorted_link = np.sort(df['link_ID_en'].unique())\n",
    "df['link_ID_en'] = df['link_ID_en'].map(lambda x: np.argmin(x >= sorted_link))\n",
    "\n",
    "def std(group):\n",
    "    group['travel_time_std'] = np.std(group['travel_time'])\n",
    "    return group\n",
    "\n",
    "df = df.groupby('link_ID').apply(std)\n",
    "df['travel_time'] = df['travel_time'] / df['travel_time_std']\n",
    "\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.2,\n",
    "    'n_estimators': 30,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 1,\n",
    "    'reg_alpha': 0,\n",
    "    'gamma': 0\n",
    "}\n",
    "\n",
    "df = pd.get_dummies(df, columns=['links_num', 'width', 'minute', 'hour', 'week_day', 'day', 'month'])\n",
    "\n",
    "print (df.head(20))\n",
    "\n",
    "feature = df.columns.values.tolist()\n",
    "train_feature = [x for x in feature if\n",
    "                 x not in ['link_ID', 'time_interval_begin', 'travel_time', 'date', 'travel_time2', 'minute_trend',\n",
    "                           'travel_time_std', 'date_trend']]\n",
    "\n",
    "train_df = df.loc[~df['travel_time'].isnull()]\n",
    "test_df = df.loc[df['travel_time'].isnull()].copy()\n",
    "\n",
    "print(train_feature)\n",
    "X = train_df[train_feature].values\n",
    "y = train_df['travel_time'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "eval_set = [(X_test, y_test)]\n",
    "regressor = xgb.XGBRegressor(learning_rate=params['learning_rate'], n_estimators=params['n_estimators'],\n",
    "                             booster='gbtree', objective='reg:linear', n_jobs=-1, subsample=params['subsample'],\n",
    "                             colsample_bytree=params['colsample_bytree'], random_state=0,\n",
    "                             max_depth=params['max_depth'], gamma=params['gamma'],\n",
    "                             min_child_weight=params['min_child_weight'], reg_alpha=params['reg_alpha'])\n",
    "regressor.fit(X_train, y_train, verbose=True, early_stopping_rounds=10, eval_set=eval_set)\n",
    "\n",
    "test_df['prediction'] = regressor.predict(test_df[train_feature].values)\n",
    "\n",
    "df = pd.merge(df, test_df[['link_ID', 'time_interval_begin', 'prediction']], on=['link_ID', 'time_interval_begin'],\n",
    "              how='left')\n",
    "\n",
    "feature_vis(regressor,train_feature)\n",
    "\n",
    "df['imputation1'] = df['travel_time'].isnull()\n",
    "df['travel_time'] = df['travel_time'].fillna(value=df['prediction'])\n",
    "df['travel_time'] = (df['travel_time'] * np.array(df['travel_time_std']) + np.array(df['minute_trend'])\n",
    "                     + np.array(df['date_trend']))\n",
    "\n",
    "def vis(group):\n",
    "    group['travel_time'].plot()\n",
    "    tmp = group.loc[group['imputation1'] == True]\n",
    "    plt.scatter(tmp.index, tmp['travel_time'], c='r')\n",
    "    plt.show()\n",
    "\n",
    "df.groupby(['link_ID', 'date']).apply(vis)\n",
    "\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<re.Match object; span=(0, 7), match='1023-23'>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "\n",
    "original_time = '1023-23-23'\n",
    "\n",
    "list1 = original_time.split(\"-\")\n",
    "\n",
    "if len(list1) > 2:\n",
    "    \n",
    "\n",
    "\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}